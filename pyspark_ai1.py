# -*- coding: utf-8 -*-
"""Pyspark_ai1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TmFU0M5UVUAxY7bgJO770lfP7NzFSJai
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
import pyspark
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
get_ipython().system_raw('./ngrok http 4050 &')
!sleep 10
!curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

!pyspark --version

"""RDD Word count Program in Pyspark"""

from google.colab import files
files.upload()

rdd =  spark.sparkContext.textFile("Word.txt")

rdd.collect()

rdd1 = rdd.flatMap(lambda x: x.split(" "));

rdd1.collect()



rdd2 = rdd1.map(lambda x: (x,1));

rdd2.collect()

rdd3 = rdd2.reduceByKey(lambda a,b:a+b)

rdd3.collect()

"""Dataframes in Pyspark"""

dataset = [("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100)
  ]
columns= ["employee_name", "department", "salary"]

df = spark.createDataFrame(data=dataset, schema=columns)

df.show()

# create duplicate tuples
distinctdf = df.distinct()
distinctdf.show()

#remove tuples with respect to one or mre column
dropcolumns_df = df.dropDuplicates(["department","salary"])
dropcolumns_df.show()

df.orderBy("department").show()

#running sql query on dataframes
df.createOrReplaceTempView("emp")
spark.sql("select employee_name, salary from emp order by  salary").show()

df.select("employee_name", "department").show()

from pyspark.sql.functions import *

df2 = df.withColumn("salary",col("salary").cast("INTEGER"))

df2.printSchema()

df3 = df.withColumn("salary",col("salary")*100)
df3.show()

df4 = df.withColumn("new_column",col("salary")*2)
df4.show()

df5 = df.withColumn("country",lit("India"))
df5.show()

"""Dataframes assignments (CSV files)"""

from google.colab import files
files.upload()

df = spark.read.csv("heroes_information.csv", header= True )

df.show()

#number of male heros
df.filter(df.Gender == "Male").count()

df.filter(df.Gender == "Female").count()

df.filter((df.Gender != "Male") & (df.Gender != "Female")).count()

#count the no of recordes in each race
df.groupby("Race").count().show()#n = df.count(),truncate = False)

print(len(df.columns))

df.count()

from google.colab import files
files.upload()

df1 = spark.read.csv("WorldCupPlayers (1).csv", header= True )

df1.show()

df1.printSchema()
df1.show()

df1.count()

print(len(df1.columns))

df1.select("Player Name").show()

df1.select("Coach Name").show()

df1.select("Coach Name").distinct().show()

df1.filter(df1.MatchID == 1096).show()

df1.filter(df1.MatchID == 1096).count()

df1.sort("MatchId").show()

df1.filter((df1.Position == "c") & (df1.Event == "G40'") ).show()

df1.select("MatchID").distinct().show()

"""JSON File Assignment"""

from google.colab import files
files.upload()

df = spark.read.json("sample_books.json")

df.show()

#show title,price,year written of books
df.select("title","price","year_written").show()

#show the books which have been written after 1950 and price of the book should be greater than 10.
df.filter((df.year_written >1950) & (df.price>10)).show()

df.select("title","year_written").filter("title LIKE '%Harry Potter%'").show()

"""Pyspark Machine learning Program"""

from google.colab import files
files.upload()

dataset = spark.read.csv("BostonHousing.csv",inferSchema= True, header= True)

dataset.show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

assembler = VectorAssembler(inputCols=['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','b','lstat'],outputCol = 'Attributes')

output = assembler.transform(dataset)

output.show()

finalized_data = output.select("Attributes","medv")
finalized_data.show()

train_data,test_data = finalized_data.randomSplit([.8,.2])

regressor = LinearRegression(featuresCol= 'Attributes', labelCol='medv')

regressor = regressor.fit(train_data)

pred = regressor.evaluate(test_data)

pred.predictions.show()